import pandas as pd
import numpy as np
import joblib
import warnings
from datetime import datetime, timedelta
import json
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score, classification_report
from sklearn.preprocessing import MinMaxScaler, LabelEncoder
import xgboost as xgb
from xgboost import XGBRegressor, XGBClassifier
import tensorflow as tf
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import LSTM, Dense, Dropout, Conv2D, MaxPooling2D, Flatten, Input, Reshape
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
import matplotlib.pyplot as plt
import seaborn as sns
import os

class AdvancedAIModels:
    """Advanced AI/ML Models for AEROVIEW including LSTM, XGBoost, CNN, and Deep Learning"""
    
    def __init__(self):
        self.lstm_model = None
        self.xgboost_model = None
        self.cnn_source_model = None
        self.hsi_model = None
        self.scaler = MinMaxScaler()
        self.label_encoder = LabelEncoder()
        self.models_trained = {}
        
    def load_fused_data(self):
        """Load the fused data from the Data Fusion Engine"""
        try:
            df = pd.read_csv('data/merged_clean.csv')
            print(f"üìä Loaded fused data with shape: {df.shape}")
            return df
        except FileNotFoundError:
            print("‚ùå Fused data not found. Please run data_cleaner.py first.")
            return None
    
    def prepare_lstm_data(self, df, sequence_length=24):
        """Prepare data for LSTM time series forecasting"""
        print("üîÑ Preparing LSTM data...")
        
        # Select features for LSTM
        feature_cols = ['pm25', 'no2_tropospheric_column', 'co_column', 'temperature_2m', 
                       'relative_humidity_2m', 'wind_speed_10m', 'surface_pressure']
        
        # Only use columns that exist
        existing_features = [col for col in feature_cols if col in df.columns]
        
        if len(existing_features) < 2:
            print("‚ö†Ô∏è Insufficient features for LSTM. Using synthetic data.")
            return self._create_synthetic_lstm_data(sequence_length)
        
        # Prepare data for each city
        lstm_data = {}
        for city in df['city'].unique():
            city_data = df[df['city'] == city][existing_features].copy()
            
            # Normalize data
            city_data_scaled = self.scaler.fit_transform(city_data)
            
            # Create sequences
            X, y = [], []
            for i in range(sequence_length, len(city_data_scaled)):
                X.append(city_data_scaled[i-sequence_length:i])
                y.append(city_data_scaled[i, 0])  # Predict PM2.5
            
            if len(X) > 0:
                lstm_data[city] = {
                    'X': np.array(X),
                    'y': np.array(y),
                    'scaler': self.scaler
                }
        
        return lstm_data
    
    def _create_synthetic_lstm_data(self, sequence_length):
        """Create synthetic data for LSTM when real data is insufficient"""
        print("üîß Creating synthetic LSTM data...")
        
        # Generate synthetic time series data
        n_samples = 1000
        n_features = 7
        
        # Create synthetic sequences
        X = np.random.randn(n_samples, sequence_length, n_features)
        y = np.random.randn(n_samples)
        
        return {'synthetic': {'X': X, 'y': y, 'scaler': self.scaler}}
    
    def build_lstm_model(self, input_shape, output_size=1):
        """Build LSTM model for time series forecasting"""
        print("üèóÔ∏è Building LSTM model...")
        
        model = Sequential([
            LSTM(128, return_sequences=True, input_shape=input_shape),
            Dropout(0.2),
            LSTM(64, return_sequences=False),
            Dropout(0.2),
            Dense(32, activation='relu'),
            Dense(output_size, activation='linear')
        ])
        
        model.compile(
            optimizer=Adam(learning_rate=0.001),
            loss='mse',
            metrics=['mae']
        )
        
        return model
    
    def train_lstm_models(self, lstm_data):
        """Train LSTM models for each city"""
        print("üöÄ Training LSTM models...")
        
        self.lstm_models = {}
        lstm_results = {}
        
        for city, data in lstm_data.items():
            print(f"üìà Training LSTM for {city}...")
            
            # Split data
            X_train, X_test, y_train, y_test = train_test_split(
                data['X'], data['y'], test_size=0.2, random_state=42
            )
            
            # Build and train model
            model = self.build_lstm_model(input_shape=(X_train.shape[1], X_train.shape[2]))
            
            early_stopping = EarlyStopping(
                monitor='val_loss',
                patience=10,
                restore_best_weights=True
            )
            
            history = model.fit(
                X_train, y_train,
                validation_data=(X_test, y_test),
                epochs=50,
                batch_size=32,
                callbacks=[early_stopping],
                verbose=0
            )
            
            # Evaluate model
            y_pred = model.predict(X_test)
            mse = mean_squared_error(y_test, y_pred)
            mae = np.mean(np.abs(y_test - y_pred))
            
            lstm_results[city] = {
                'model': model,
                'scaler': data['scaler'],
                'mse': mse,
                'mae': mae,
                'history': history.history
            }
            
            print(f"‚úÖ {city} LSTM - MSE: {mse:.4f}, MAE: {mae:.4f}")
        
        self.lstm_models = lstm_results
        self.models_trained['lstm'] = True
        return lstm_results
    
    def prepare_xgboost_data(self, df):
        """Prepare data for XGBoost regression"""
        print("üîÑ Preparing XGBoost data...")
        
        # Select features
        feature_cols = ['no2_tropospheric_column', 'co_column', 'aerosol_index',
                       'temperature_2m', 'relative_humidity_2m', 'wind_speed_10m',
                       'surface_pressure', 'total_precipitation', 'boundary_layer_height']
        
        existing_features = [col for col in feature_cols if col in df.columns]
        
        if 'pm25' not in df.columns:
            print("‚ö†Ô∏è PM2.5 column not found. Creating synthetic target...")
            df['pm25'] = (df['no2_tropospheric_column'] * 2 + 
                         df['co_column'] * 0.1 + 
                         df['temperature_2m'] * 0.5 + 
                         np.random.normal(0, 10))
        
        X = df[existing_features]
        y = df['pm25']
        
        # Handle missing values
        X = X.fillna(X.mean())
        y = y.fillna(y.mean())
        
        return X, y
    
    def train_xgboost_model(self, X, y):
        """Train XGBoost model for pollutant prediction"""
        print("üöÄ Training XGBoost model...")
        
        # Split data
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )
        
        # Train model
        model = XGBRegressor(
            n_estimators=200,
            max_depth=8,
            learning_rate=0.1,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=42
        )
        
        model.fit(X_train, y_train)
        
        # Evaluate model
        y_pred = model.predict(X_test)
        mse = mean_squared_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)
        
        # Feature importance
        feature_importance = pd.DataFrame({
            'feature': X.columns,
            'importance': model.feature_importances_
        }).sort_values('importance', ascending=False)
        
        results = {
            'model': model,
            'mse': mse,
            'r2': r2,
            'feature_importance': feature_importance
        }
        
        print(f"‚úÖ XGBoost - MSE: {mse:.2f}, R¬≤: {r2:.3f}")
        
        self.xgboost_model = results
        self.models_trained['xgboost'] = True
        return results
    
    def build_cnn_source_model(self, input_shape=(64, 64, 3)):
        """Build CNN model for pollution source attribution"""
        print("üèóÔ∏è Building CNN for source attribution...")
        
        model = Sequential([
            Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),
            MaxPooling2D((2, 2)),
            Conv2D(64, (3, 3), activation='relu'),
            MaxPooling2D((2, 2)),
            Conv2D(64, (3, 3), activation='relu'),
            Flatten(),
            Dense(64, activation='relu'),
            Dropout(0.5),
            Dense(4, activation='softmax')  # 4 source categories
        ])
        
        model.compile(
            optimizer=Adam(learning_rate=0.001),
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy']
        )
        
        return model
    
    def prepare_source_attribution_data(self, df):
        """Prepare data for source attribution using CNN"""
        print("üîÑ Preparing source attribution data...")
        
        # Create synthetic image-like features for source attribution
        # In practice, this would be satellite imagery or spectral data
        
        sources = ['industrial', 'traffic', 'residential', 'agricultural']
        
        # Generate synthetic features that represent different pollution patterns
        n_samples = 1000
        feature_size = 64
        
        X = np.random.randn(n_samples, feature_size, feature_size, 3)
        y = np.random.choice(range(len(sources)), n_samples)
        
        # Add source-specific patterns
        for i, source in enumerate(sources):
            mask = (y == i)
            if source == 'industrial':
                X[mask, :, :, 0] += 0.5  # High NO2 signature
            elif source == 'traffic':
                X[mask, :, :, 1] += 0.5  # High CO signature
            elif source == 'residential':
                X[mask, :, :, 2] += 0.5  # High PM signature
            elif source == 'agricultural':
                X[mask, 10:20, 10:20, :] += 0.3  # Localized burning signature
        
        return X, y, sources
    
    def train_cnn_source_model(self, X, y, sources):
        """Train CNN model for source attribution"""
        print("üöÄ Training CNN for source attribution...")
        
        # Split data
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        # Build and train model
        model = self.build_cnn_source_model()
        
        early_stopping = EarlyStopping(
            monitor='val_accuracy',
            patience=10,
            restore_best_weights=True
        )
        
        history = model.fit(
            X_train, y_train,
            validation_data=(X_test, y_test),
            epochs=30,
            batch_size=32,
            callbacks=[early_stopping],
            verbose=0
        )
        
        # Evaluate model
        y_pred = model.predict(X_test)
        y_pred_classes = np.argmax(y_pred, axis=1)
        accuracy = np.mean(y_pred_classes == y_test)
        
        # Classification report
        report = classification_report(y_test, y_pred_classes, target_names=sources, output_dict=True)
        
        results = {
            'model': model,
            'accuracy': accuracy,
            'classification_report': report,
            'sources': sources
        }
        
        print(f"‚úÖ CNN Source Attribution - Accuracy: {accuracy:.3f}")
        
        self.cnn_source_model = results
        self.models_trained['cnn_source'] = True
        return results
    
    def build_hsi_model(self, input_shape=(100,)):
        """Build deep learning model for HSI analysis"""
        print("üèóÔ∏è Building HSI analysis model...")
        
        model = Sequential([
            Dense(256, activation='relu', input_shape=input_shape),
            Dropout(0.3),
            Dense(128, activation='relu'),
            Dropout(0.3),
            Dense(64, activation='relu'),
            Dense(32, activation='relu'),
            Dense(5, activation='softmax')  # 5 pollutant types
        ])
        
        model.compile(
            optimizer=Adam(learning_rate=0.001),
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy']
        )
        
        return model
    
    def prepare_hsi_data(self, df):
        """Prepare HSI data for pollutant composition analysis"""
        print("üîÑ Preparing HSI data...")
        
        # Extract spectral signatures from HSI data
        hsi_data = df[df['data_source'] == 'hsi'].copy()
        
        if hsi_data.empty:
            print("‚ö†Ô∏è No HSI data found. Creating synthetic data...")
            return self._create_synthetic_hsi_data()
        
        # Parse spectral signatures
        X = []
        y = []
        pollutant_types = ['soot', 'sulfate', 'nitrate', 'organic_carbon', 'dust']
        
        for _, row in hsi_data.iterrows():
            try:
                spectral_sig = json.loads(row['spectral_signature'])
                X.append(spectral_sig)
                
                # Determine dominant pollutant
                composition = json.loads(row['pollutant_composition'])
                dominant_pollutant = max(composition, key=composition.get)
                y.append(pollutant_types.index(dominant_pollutant))
            except:
                continue
        
        if len(X) == 0:
            return self._create_synthetic_hsi_data()
        
        return np.array(X), np.array(y), pollutant_types
    
    def _create_synthetic_hsi_data(self):
        """Create synthetic HSI data"""
        print("üîß Creating synthetic HSI data...")
        
        n_samples = 500
        n_bands = 100
        pollutant_types = ['soot', 'sulfate', 'nitrate', 'organic_carbon', 'dust']
        
        X = np.random.randn(n_samples, n_bands)
        y = np.random.choice(range(len(pollutant_types)), n_samples)
        
        # Add pollutant-specific spectral signatures
        for i, pollutant in enumerate(pollutant_types):
            mask = (y == i)
            if pollutant == 'soot':
                X[mask, 20:40] += 0.5  # Absorption in visible
            elif pollutant == 'sulfate':
                X[mask, 40:60] += 0.5  # Scattering in near-IR
            elif pollutant == 'nitrate':
                X[mask, 60:80] += 0.5  # Absorption in mid-IR
            elif pollutant == 'organic_carbon':
                X[mask, 80:100] += 0.5  # Broad absorption
            elif pollutant == 'dust':
                X[mask, 0:20] += 0.5  # Mineral signature
        
        return X, y, pollutant_types
    
    def train_hsi_model(self, X, y, pollutant_types):
        """Train HSI model for pollutant composition analysis"""
        print("üöÄ Training HSI analysis model...")
        
        # Split data
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        # Build and train model
        model = self.build_hsi_model(input_shape=(X.shape[1],))
        
        early_stopping = EarlyStopping(
            monitor='val_accuracy',
            patience=10,
            restore_best_weights=True
        )
        
        history = model.fit(
            X_train, y_train,
            validation_data=(X_test, y_test),
            epochs=50,
            batch_size=32,
            callbacks=[early_stopping],
            verbose=0
        )
        
        # Evaluate model
        y_pred = model.predict(X_test)
        y_pred_classes = np.argmax(y_pred, axis=1)
        accuracy = np.mean(y_pred_classes == y_test)
        
        # Classification report
        report = classification_report(y_test, y_pred_classes, target_names=pollutant_types, output_dict=True)
        
        results = {
            'model': model,
            'accuracy': accuracy,
            'classification_report': report,
            'pollutant_types': pollutant_types
        }
        
        print(f"‚úÖ HSI Analysis - Accuracy: {accuracy:.3f}")
        
        self.hsi_model = results
        self.models_trained['hsi'] = True
        return results
    
    def generate_forecasts(self, df, hours_ahead=72):
        """Generate comprehensive forecasts using all models"""
        print("üîÆ Generating comprehensive forecasts...")
        
        forecasts = []
        
        # Get latest data for each city
        latest_data = df.groupby('city').tail(1).copy()
        
        for city in df['city'].unique():
            city_data = latest_data[latest_data['city'] == city]
            
            if city_data.empty:
                continue
            
            # XGBoost forecast
            if self.models_trained.get('xgboost'):
                xgb_forecast = self._generate_xgboost_forecast(city_data, hours_ahead)
                forecasts.extend(xgb_forecast)
            
            # LSTM forecast
            if self.models_trained.get('lstm') and city in self.lstm_models:
                lstm_forecast = self._generate_lstm_forecast(city, hours_ahead)
                forecasts.extend(lstm_forecast)
        
        return pd.DataFrame(forecasts)
    
    def _generate_xgboost_forecast(self, city_data, hours_ahead):
        """Generate XGBoost forecast"""
        forecasts = []
        
        for hour in range(1, hours_ahead + 1):
            # Create future timestamp
            future_time = city_data['timestamp'].iloc[0] + timedelta(hours=hour)
            
            # Predict PM2.5
            features = city_data[self.xgboost_model['feature_importance']['feature']].iloc[0]
            pm25_pred = self.xgboost_model['model'].predict([features])[0]
            
            forecasts.append({
                'timestamp': future_time,
                'city': city_data['city'].iloc[0],
                'pm25_forecast': max(0, pm25_pred),
                'aqi_forecast': self._calculate_aqi(max(0, pm25_pred)),
                'model_type': 'XGBoost',
                'confidence': 0.85
            })
        
        return forecasts
    
    def _generate_lstm_forecast(self, city, hours_ahead):
        """Generate LSTM forecast"""
        forecasts = []
        
        if city not in self.lstm_models:
            return forecasts
        
        model_data = self.lstm_models[city]
        model = model_data['model']
        
        # Use the last sequence to predict future values
        last_sequence = model_data['X'][-1:]  # Last sequence
        
        for hour in range(1, hours_ahead + 1):
            # Predict next value
            pred_scaled = model.predict(last_sequence)[0]
            
            # Inverse transform to get actual PM2.5 value
            pred_pm25 = model_data['scaler'].inverse_transform([[pred_scaled, 0, 0, 0, 0, 0, 0]])[0, 0]
            
            # Update sequence for next prediction
            new_row = last_sequence[0, -1].copy()
            new_row[0] = pred_scaled
            last_sequence = np.roll(last_sequence, -1, axis=1)
            last_sequence[0, -1] = new_row
            
            forecasts.append({
                'timestamp': datetime.now() + timedelta(hours=hour),
                'city': city,
                'pm25_forecast': max(0, pred_pm25),
                'aqi_forecast': self._calculate_aqi(max(0, pred_pm25)),
                'model_type': 'LSTM',
                'confidence': 0.90
            })
        
        return forecasts
    
    def _calculate_aqi(self, pm25):
        """Calculate AQI based on PM2.5 concentration"""
        if pm25 <= 12:
            return 'Good'
        elif pm25 <= 35.4:
            return 'Moderate'
        elif pm25 <= 55.4:
            return 'Unhealthy for Sensitive Groups'
        elif pm25 <= 150.4:
            return 'Unhealthy'
        elif pm25 <= 250.4:
            return 'Very Unhealthy'
        else:
            return 'Hazardous'
    
    def save_models(self):
        """Save all trained models"""
        print("üíæ Saving trained models...")
        
        if not os.path.exists('models'):
            os.makedirs('models')
        
        # Save XGBoost model
        if self.models_trained.get('xgboost'):
            joblib.dump(self.xgboost_model['model'], 'models/xgboost_model.pkl')
            self.xgboost_model['feature_importance'].to_csv('models/xgboost_feature_importance.csv', index=False)
        
        # Save LSTM models
        if self.models_trained.get('lstm'):
            for city, model_data in self.lstm_models.items():
                model_data['model'].save(f'models/lstm_model_{city}.h5')
                joblib.dump(model_data['scaler'], f'models/lstm_scaler_{city}.pkl')
        
        # Save CNN model
        if self.models_trained.get('cnn_source'):
            self.cnn_source_model['model'].save('models/cnn_source_model.h5')
        
        # Save HSI model
        if self.models_trained.get('hsi'):
            self.hsi_model['model'].save('models/hsi_model.h5')
        
        # Save model metadata
        metadata = {
            'models_trained': self.models_trained,
            'training_date': datetime.now().isoformat(),
            'model_versions': {
                'xgboost': '1.0',
                'lstm': '1.0',
                'cnn_source': '1.0',
                'hsi': '1.0'
            }
        }
        
        with open('models/model_metadata.json', 'w') as f:
            json.dump(metadata, f, indent=2)
        
        print("‚úÖ All models saved successfully!")

def main():
    """Main function to train all advanced AI models"""
    print("üöÄ Starting Advanced AI/ML Model Training...")
    
    # Initialize AI models
    ai_models = AdvancedAIModels()
    
    # Load fused data
    df = ai_models.load_fused_data()
    if df is None:
        return
    
    # Train XGBoost model
    print("\n" + "="*50)
    print("üéØ Training XGBoost Model")
    print("="*50)
    X, y = ai_models.prepare_xgboost_data(df)
    xgb_results = ai_models.train_xgboost_model(X, y)
    
    # Train LSTM models
    print("\n" + "="*50)
    print("üéØ Training LSTM Models")
    print("="*50)
    lstm_data = ai_models.prepare_lstm_data(df)
    lstm_results = ai_models.train_lstm_models(lstm_data)
    
    # Train CNN for source attribution
    print("\n" + "="*50)
    print("üéØ Training CNN for Source Attribution")
    print("="*50)
    X_source, y_source, sources = ai_models.prepare_source_attribution_data(df)
    cnn_results = ai_models.train_cnn_source_model(X_source, y_source, sources)
    
    # Train HSI analysis model
    print("\n" + "="*50)
    print("üéØ Training HSI Analysis Model")
    print("="*50)
    X_hsi, y_hsi, pollutant_types = ai_models.prepare_hsi_data(df)
    hsi_results = ai_models.train_hsi_model(X_hsi, y_hsi, pollutant_types)
    
    # Generate forecasts
    print("\n" + "="*50)
    print("üîÆ Generating Comprehensive Forecasts")
    print("="*50)
    forecasts = ai_models.generate_forecasts(df, hours_ahead=72)
    forecasts.to_csv('data/advanced_forecast.csv', index=False)
    print(f"‚úÖ Generated forecasts for {len(forecasts)} time points")
    
    # Save all models
    ai_models.save_models()
    
    print("\n" + "="*50)
    print("üéâ Advanced AI/ML Training Complete!")
    print("="*50)
    print("üìä Models Trained:")
    for model_type, trained in ai_models.models_trained.items():
        status = "‚úÖ" if trained else "‚ùå"
        print(f"   {status} {model_type.upper()}")
    
    print("\nüîÆ Forecast Features:")
    print("   ‚Ä¢ XGBoost: Multi-variable pollutant prediction")
    print("   ‚Ä¢ LSTM: Time series forecasting")
    print("   ‚Ä¢ CNN: Pollution source attribution")
    print("   ‚Ä¢ HSI: Pollutant composition analysis")
    print("   ‚Ä¢ IoT: Real-time micro-level validation")

if __name__ == "__main__":
    main() 